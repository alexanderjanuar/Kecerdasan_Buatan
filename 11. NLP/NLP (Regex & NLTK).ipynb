{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regular expression & word tokenization"
      ],
      "metadata": {
        "id": "hNHV54UF-BrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regex\n",
        "\n",
        "Regex adalah sebuah **string** dengan syntax spesial yang digunakan untuk menemukan sebuah pola didalam suatu string"
      ],
      "metadata": {
        "id": "onYlPYVm_xhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re #Libray Regex"
      ],
      "metadata": {
        "id": "xFpegEw2MPzP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beberapa Contoh Pola Regex yang umum**\n",
        "\n",
        "Pola | Fungsi | Contoh\n",
        "--- | --- | ---\n",
        "\\+w | Kata | 'Regex'\n",
        "\\d | Digit | 9\n",
        "\\s | Spasi | ' '\n",
        ".* | Wildcard | 'username74'\n",
        "\\S | Bukan Spasi | 'bukan_spasi'\n",
        "[a-z] | Kelompok huruf kecil | 'abcdefg'\n"
      ],
      "metadata": {
        "id": "0izxrjRkAdXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Latihan : Gunakan fungsi regex re.split() dan re.findall()"
      ],
      "metadata": {
        "id": "W39qELysCeu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
      ],
      "metadata": {
        "id": "GArVxdhl__HE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdmFq7FECxZ0",
        "outputId": "2cfc11ce-4f81-4096-c147-a433152a6caf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cari semua kalimat kapital didalam string\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM51OKjiC-eB",
        "outputId": "9e0f6973-e8fa-4128-e4a8-06107665f02e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', 'RegEx', 'Won', 'Can', 'Or']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pisah string dengan spasi\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w98yNExDKX6",
        "outputId": "b804f3a4-0f90-4ac7-e2cf-5cffd0194dc7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cari semua digit didalam string\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6R-YZ70DRTY",
        "outputId": "3ee3a67e-90cb-4b95-ce52-3ff6e2ee5c82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['4', '19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "- Mengubah string atau dokumen menjadi sebuah **token** (Bagian lebih kecil)\n",
        "- Salah satu tahap dalam pemrosesan text untuk NLP\n",
        "- Beberapa contoh:\n",
        "  - Memecah sebuah kata atau kalimat\n",
        "  - Memisahkan tanda baca\n",
        "  - Memisahkan **hashtag** pada suatu tweet\n"
      ],
      "metadata": {
        "id": "kgG9BVOXIrW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ntlk : Natural language toolkit\n",
        "NTLK adalah library dari python yang digunakan untuk melakukan tokenisasi"
      ],
      "metadata": {
        "id": "Vl6sYkDSJzkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "5GnRYUIQNp9X",
        "outputId": "54444d72-de37-4eb0-ef1b-2ff44c5d49d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "\n",
        "word_tokenize(\"Hi There!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNpxMaxbDY6M",
        "outputId": "010a9b6a-ccef-4c2f-9298-b637edd9458e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi', 'There', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word tokenize**: Kita menggunakan word_tokenize() membagi sebuah kalimat menjadi token atau kata\n",
        "\n",
        "**Sentence tokenize**: Kita menggunakan sent_tokenize() untuk membagi dokumen atau paragraf menjadi suatu kalimat"
      ],
      "metadata": {
        "id": "noK7HSHtVBuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word tokenization with NLTK"
      ],
      "metadata": {
        "id": "_Ci7z8XhUaMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scene_one = '''\n",
        "\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\n",
        "        '''\n",
        "\n",
        "# Pisah scene one menjadi 1 kalimat: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "sentences[0:5]"
      ],
      "metadata": {
        "id": "GDZt-BpNNH99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86e1afe-72ea-45d7-82f5-7b59364cca87"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!',\n",
              " '[clop clop clop] \\nSOLDIER #1: Halt!',\n",
              " 'Who goes there?',\n",
              " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.',\n",
              " 'King of the Britons, defeator of the Saxons, sovereign of all England!']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gunakan word_tokenize untuk mengtokenasi kalimat ke 4: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "tokenized_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DauLzKVKUZLa",
        "outputId": "3a215022-d29f-4729-b52e-db52884ddb57"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ARTHUR',\n",
              " ':',\n",
              " 'It',\n",
              " 'is',\n",
              " 'I',\n",
              " ',',\n",
              " 'Arthur',\n",
              " ',',\n",
              " 'son',\n",
              " 'of',\n",
              " 'Uther',\n",
              " 'Pendragon',\n",
              " ',',\n",
              " 'from',\n",
              " 'the',\n",
              " 'castle',\n",
              " 'of',\n",
              " 'Camelot',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat kumpulan token unik dari scene_one : unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "print(f'Jumlah Kata-kata unik didalam kalimat : {len(unique_tokens)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnR94hLNV9JL",
        "outputId": "21c8d0a0-607f-49d2-b0a3-e361c493c511"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah Kata-kata unik didalam kalimat : 227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More regex with re.search()"
      ],
      "metadata": {
        "id": "7QlGkYlLXDoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cari kemunculan pertama dari kata 'coconut': match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(f\"Kalimat berawal pada index  : {match.start()}\")\n",
        "print(f\"Kalimat berakhir pada index : {match.end()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhumyHEIWN2p",
        "outputId": "542160b5-7ba3-4518-dbe7-1c2b1e6ab180"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kalimat berawal pada index  : 582\n",
            "Kalimat berakhir pada index : 590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regex with NLTK tokenization"
      ],
      "metadata": {
        "id": "CUNkPlnr5pt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "ZAy6f2L3XoOF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = ['This is the best #nlp exercise ive found online! #python','#NLP is super fun! <3 #learning','Thanks @ :) #nlp #python']"
      ],
      "metadata": {
        "id": "9Of-0IT95usr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat pola regex untuk mencari hashtag: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "hashtags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h5FL5896JQZ",
        "outputId": "f77d9ffd-bace-4a2c-d72f-f891374e9353"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#nlp', '#python']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gunakan TweetTokenizer untuk mengtokenasi tweets menjadi 1 list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eToagfLQ5-bk",
        "outputId": "6ee64716-3565-48d8-ffbe-7d2be521dbc1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@', ':)', '#nlp', '#python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple topic identification**"
      ],
      "metadata": {
        "id": "zcy3d6EbH0og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word counts with bag-of-words\n",
        "- Metode dasar yang digunakan untuk mencari sebuah topik didalam teks\n",
        "- Diharuskan membuat sebuah token melalui tokenisasi\n",
        "- .. lalu menghitung semua token yang ada\n",
        "- Semakin sering sebuah kata muncul, maka semakin penting kata tersebut\n",
        "- Digunakan untuk menentukan kata yang signifikan didalam sebuah text"
      ],
      "metadata": {
        "id": "8kfSEgfaH3wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"I pledge to be a data scientist one day one day data scientist\"\n",
        "tokenized_text=word_tokenize(data)"
      ],
      "metadata": {
        "id": "CKhffRB0H3QO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Ubah token menjadi huruf kecil: lower_tokens\n",
        "lower_tokens = [i.lower() for i in tokenized_text]\n",
        "\n",
        "# Gunakan library counter: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print 5 kata yang paling sering muncul\n",
        "print(bow_simple.most_common(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AA1girsIhiZ",
        "outputId": "3ffd40aa-9fb0-473d-da85-079fdd9c6a97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('data', 2), ('scientist', 2), ('one', 2), ('day', 2), ('i', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simple text preprocessing\n",
        "**Mengapa preprocess?**\n",
        "- Memperbaiki input data\n",
        "  - Ketika melakukan metode *machine learning* atau metode statistikal lainnnya\n",
        "- Contoh:\n",
        "  - Tokenisasi kalimat untuk membuat *bag of words*\n",
        "  - *Lowercasing* sebuah kata\n",
        "- Stemming & Lemmatization\n",
        "  - **Stemming**\n",
        "    1. Stemming merupakan proses untuk menghapus beberapa karakter terakhir dari sebuah kata, kadang dapat menyebabkan kesalahan dalam pengejaan\n",
        "    2. Contoh, *stemming* sebuah kata **Caring** akan mengembalikan kata **Car**\n",
        "    3. Stemming digunakan pada dataset yang besar\n",
        "\n",
        "  - **Lemmatization**\n",
        "    1. **Lemmatization** mengubah suatu kata menjadi betuk dasarnya (Lemma)\n",
        "    2. Contoh, **Caring** akan mengembalikan kata **Care**\n",
        "    3. Berat dalam komputasi\n",
        "- Menghapus kata umum (ex : the, is, in,..) , kata sambung, atau token yang tidak diinginkan\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M0LAZxPeKXlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc1p1mZnr23E",
        "outputId": "a9d7dcd5-c322-4b53-8578-56b9a4137306"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cat is in the box. The cat likes the box.  The box is over the cat.\"\n",
        "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] #Ambil alphabet didalam kalimat\n",
        "\n",
        "no_stops = [t for t in tokens if t not in stopwords.words('english')] # Hapus stopwords (the,is,in,..)\n",
        "\n",
        "Counter(no_stops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzi1LnD1Js-z",
        "outputId": "93ed61f5-f634-41bc-947d-59069243d3d7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'cat': 3, 'box': 3, 'likes': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ambil hanya kata alphabet: alpha_only\n",
        "alpha_only = [t for t in tokenized_sent if t.isalpha()]\n",
        "\n",
        "# Hapus stopwords (the,is,in,..): no_stops\n",
        "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
        "\n",
        "# Buat objek WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize semua token menjadi satu list yang baru: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOWaIQi1sB0k",
        "outputId": "2837c49c-d589-461c-9597-392ee6d5407f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ARTHUR', 1), ('It', 1), ('I', 1), ('Arthur', 1), ('son', 1), ('Uther', 1), ('Pendragon', 1), ('castle', 1), ('Camelot', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Named Entity Recognition**\n",
        "- Merupakan suatu proses didalam NLP yang digunakan untuk mengindentifkasi entitas penting didalam text\n",
        "  - Nama orang, organisasi, tempat\n",
        "  - Tanggal, dll (Tergantung dari library atau notasi yang kita gunakan)\n",
        "- Dapat digunakan dalam *topic identification*\n",
        "- Dapat menjawab pertanyaan (Siapa,Apa,Kapan,Dimana)\n",
        " "
      ],
      "metadata": {
        "id": "VR9pH9H85TCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article = '''The Universitas Mulawarman is a public university located in Samarinda, East Kalimantan, Indonesia. \n",
        "It was established on September 27, 1962, making it the oldest tertiary education institution in East Kalimantan. With more than 35,000 students, \n",
        "Universitas Mulawarman is the university with the most students in Kalimantan. \n",
        "Its main campus is in Gunung Kelua, while other campuses are in Pahlawan Road, Banggeris Street and Flores Street of Samarinda.\n",
        "The name \"Mulawarman\" is taken from the legendary king Mulavarman Nala Dewa of the Kutai Martadipura Kingdom who ruled in the 4th-5th century CE, \n",
        "historically one of the earliest Hindu kingdoms in Indonesia, located in current Kutai Kartanegara Regency, East Kalimantan.'''\n",
        "\n",
        "# Tokenisasi article menjadi kalimat: sentences\n",
        "sentences = sent_tokenize(article)\n",
        "\n",
        "# Tokenisasi setiap kalimat menjadi kata : token_sentences\n",
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
        "\n",
        "\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n",
        "\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)"
      ],
      "metadata": {
        "id": "QXxiKIkA5fb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7694cf83-2c03-4531-ceeb-3b8c1dc913f9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(NE Universitas/NNP Mulawarman/NNP)\n",
            "(NE Samarinda/NNP)\n",
            "(NE East/NNP Kalimantan/NNP)\n",
            "(NE Indonesia/NNP)\n",
            "(NE East/NNP Kalimantan/NNP)\n",
            "(NE Universitas/NNP Mulawarman/NNP)\n",
            "(NE Kalimantan/NNP)\n",
            "(NE Gunung/NNP Kelua/NNP)\n",
            "(NE Pahlawan/NNP Road/NNP)\n",
            "(NE Banggeris/NNP Street/NNP)\n",
            "(NE Flores/NNP Street/NNP)\n",
            "(NE Samarinda/NNP)\n",
            "(NE Mulavarman/NNP Nala/NNP Dewa/NNP)\n",
            "(NE Kutai/NNP Martadipura/NNP Kingdom/NNP)\n",
            "(NE Indonesia/NNP)\n",
            "(NE Kutai/NNP Kartanegara/NNP Regency/NNP)\n",
            "(NE East/NNP Kalimantan/NNP)\n"
          ]
        }
      ]
    }
  ]
}